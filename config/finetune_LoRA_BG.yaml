out_dir: "small_model_untouched" # path to model ckpt to train on
eval_interval: 10  # how often to evaluate against the validation set
eval_iters_train: 10
eval_iters_val: 10
log_interval: 2  # how often to print to the console (1 = every iteration)
init_from: "resume"

# whether to always save a checkpoint
always_save_checkpoint: True
ckpt_out_dir: "crystallm_v1_small/BG_LoRA"  # the path to the directory to save the checkpoints

dataset: "BG_cifs_more_tokens"
batch_size: 4
block_size: 1024  # context of up to `block_size` previous characters

# architecture
n_layer: 6
n_head: 6
n_embd: 384
dropout: 0.2

learning_rate: 5e-5
max_iters: 50
lr_decay_iters: 50  # make equal to max_iters usually
min_lr: 5e-6  # learning_rate / 10 usually
beta2: 0.99  # make a bit bigger because number of tokens per iter is small

warmup_iters: 10  # not super necessary potentially

finetune_method : "LoRA"

# on macbook also add
# device: 'cpu'  # run on cpu only
# compile: False # do not torch compile the model
